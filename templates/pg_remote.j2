#!/bin/bash


export AWS_SHARED_CREDENTIALS_FILE=/etc/backup/credentials

usage() { 
    echo "Usage: $0 -n <archive name> -d <destination path> -a <aws endpoint> [-t <tmp dir>] [-r <retain count>] [-e <exp date>] ; -e example, \"2 min ago\", \"-2 months 5 day ago\" ";
    exit 1; 
}

while getopts ":n:d:t:r:e:a:" o; do
    case "${o}" in
        n)
            NAME=${OPTARG}
            ;;
        d)
            DEST=${OPTARG}
            ;;
        t)
            TMP=${OPTARG}
            ;;
        r)
            RETAIN_CNT=${OPTARG}
            ;;
        e)
            EXP_DATE=${OPTARG}
            ;;
        
        f)  
            BACKUP_DIR=${OPTARG}
            ;;
        
        a)  
            AWS_ENDPOINT=${OPTARG}
            ;;

        *)
            usage
            ;;
    esac
done
shift $((OPTIND-1))

if [[ -z "${TMP}" ]]; then
    TMP=/tmp
fi

if [[ -z "${RETAIN_CNT}" ]]; then
    RETAIN_CNT=0
fi

#file lock
LOCKFILE="/var/run/backup_${NAME}.lock"
TIMEOUT=1
touch $LOCKFILE
exec {FD}<>$LOCKFILE

if ! flock -x -w $TIMEOUT $FD; then
    echo "fail to lock"
    exit 1;
fi

send_notification()
{   
    if [ -f "/usr/local/bin/apprise" ] && [ -f "/etc/backup/apprise_config" ]; then
            /usr/local/bin/apprise  -t "${1}" -b "${2}" --config=/etc/backup/apprise_config
    fi
    exit 1
}

#Create backup
pg_dump postgresql://{{ pg_remote.database_user }}:{{ pg_remote.database_password }}@{{ pg_remote.database_host }}:{{ pg_remote.database_port }}/{{ pg_remote.database_name }} | gzip > ${TMP}/${NAME}.tar.gz
CREATE_RESULT=$?
if [[ $CREATE_RESULT  -gt 0 ]];  then
  sleep 600
  echo "Attempt #2"
  pg_dump postgresql://{{ pg_remote.database_user }}:{{ pg_remote.database_password }}@{{ pg_remote.database_host }}:{{ pg_remote.database_port }}/{{ pg_remote.database_name }} | gzip > ${TMP}/${NAME}.tar.gz
fi
CREATE_RESULT=$?
if [[ $CREATE_RESULT  -gt 0 ]];  then
  sleep 600
  echo "Attempt #3"
  pg_dump postgresql://{{ pg_remote.database_user }}:{{ pg_remote.database_password }}@{{ pg_remote.database_host }}:{{ pg_remote.database_port }}/{{ pg_remote.database_name }} | gzip > ${TMP}/${NAME}.tar.gz
fi
CREATE_RESULT=$?
if [[ $CREATE_RESULT  -gt 0 ]];  then
     rm ${TMP}/${NAME}.tar.gz
     echo "Backup creation failed"
     send_notification "Failed to upload tar archive" "Could't to upload tar ${NAME} to ${DEST} with exit code ${AWS_RESULT}"
     exit 1;
fi
#Upload to S3. Notification telegram
CURRENT_DATE=$(date -u +%Y%m%d%H%M%S)
ARCHIVE_BASE_NAME=base-${CURRENT_DATE}.tar.gz
ARCHIVE_WAL_NAME=wal-${CURRENT_DATE}.tar.gz

aws --endpoint-url=${AWS_ENDPOINT} s3 mv ${TMP}/${NAME}.tar.gz ${DEST}/${ARCHIVE_BASE_NAME}
AWS_RESULT=$?
if [[ $AWS_RESULT  -gt 0 ]];  then
     rm ${TMP}/${NAME}.tar.gz
     send_notification "Failed to upload tar archive" "Could't to upload tar ${NAME} to ${DEST} with exit code ${AWS_RESULT}"
fi

#Rotation by count
if [[ $RETAIN_CNT -gt 0 ]]; then
    REMOTE_FILES=$(aws --endpoint-url=${AWS_ENDPOINT} s3 ls ${DEST} | sort | awk '{print $4}' | grep ${NAME}) 
    echo "$REMOTE_FILES" | grep -v "`echo \"$REMOTE_FILES\" | tail -n ${RETAIN_CNT}`" | \
    while read file; do \
        aws --endpoint-url=${AWS_ENDPOINT} s3 rm "${DEST}${file}"; \
    done
    echo "Done"
fi

#Rotation by time
if [[ -n "${EXP_DATE}" ]]; then
     REMOTE_FILES=$(aws --endpoint-url=${AWS_ENDPOINT} s3 ls "${DEST}") 
     echo  "$REMOTE_FILES" | awk '{print $4}'  | \
  while read file; do \
        CURRENT_ARCHIEVE=$(echo $file | sed  "s/[^0-9]//g")  \
        FILTER_DATA=$(echo $(date --date="${EXP_DATE}" +%Y%m%d%H%M%S) | tr -d '-' )  
  if  [[ $CURRENT_ARCHIEVE < $FILTER_DATA  ]]; then  
          aws --endpoint-url=${AWS_ENDPOINT} s3 rm "${DEST}${file}"; 
  fi 
  done
fi
